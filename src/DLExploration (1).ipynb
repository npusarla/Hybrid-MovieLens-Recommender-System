{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DLExploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "msa29HqWPS9s"
      },
      "source": [
        "!pip install -U keras-tuner\n",
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import sklearn.preprocessing\n",
        "import kerastuner as kt\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Reshape, Dot\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from keras.layers import Add, Activation, Lambda\n",
        "from keras.layers import Concatenate, Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtIM0gQbex9x"
      },
      "source": [
        "## Model Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcVm8_dSblg_"
      },
      "source": [
        "We take in the active_users dataset and the movies dataset. We obtain the number of unique users, unique movies, the minimum rating, as well as the maximum rating as these will be used for the model's embedding layers and output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gddR7LG2G2Zp"
      },
      "source": [
        "active=pd.read_csv('active_users.csv')\n",
        "movies=pd.read_csv('movies.csv')\n",
        "\n",
        "n_users = active['userId'].nunique()\n",
        "n_movies = active['movieId'].nunique()\n",
        "min_rating = min(active['rating'])\n",
        "max_rating = max(active['rating'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfDNzH6cb3Sd"
      },
      "source": [
        "Next, we join the two datasets together, and apply a multi-label binarizer to the genres column. The user ID and movie ID columns are label encoded. We return arrays containing the inputs to the model, as well dataframes containing the original user and movie labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIhrhypFGM46"
      },
      "source": [
        "def active_preprocessing(active_user_dataset, movies_data):\n",
        "    merged=active_user_dataset.merge(movies_data).drop(['timestamp','title'], axis=1)\n",
        "    genres_encoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
        "    merged['genres'] = genres_encoder.fit_transform(merged['genres'].apply(lambda s: s.split(\"|\"))).tolist()\n",
        "    merged[genres_encoder.classes_.tolist()] = pd.DataFrame(merged.genres.tolist(), index= merged.index)\n",
        "    merged=merged.drop('genres',axis=1)\n",
        "    \n",
        "    user_enc = LabelEncoder()\n",
        "    merged['user'] = user_enc.fit_transform(merged['userId'].values)\n",
        "    item_enc = LabelEncoder()\n",
        "    merged['movie'] = item_enc.fit_transform(merged['movieId'].values)\n",
        "    merged['rating'] = merged['rating'].values.astype(np.float32)\n",
        "    \n",
        "\n",
        "    X = merged[[c for c in merged.columns if c!='rating']].values\n",
        "    y = merged['rating'].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=merged.user)\n",
        "\n",
        "    X_train_array = [X_train[:, -2], X_train[:, -1], X_train[:, 2:-2]]\n",
        "    X_test_array = [X_test[:, -2], X_test[:, -1], X_test[:, 2:-2]]\n",
        "    \n",
        "    return X_train_array, X_test_array, y_train, y_test, X_test, X_train"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n73dD5jKGRfg"
      },
      "source": [
        "X_train_array, X_test_array, y_train, y_test, X_test, X_train= active_preprocessing(active, movies)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56UuVtSlcYiR"
      },
      "source": [
        "We define an implementation of the wide and deep model below. Using keras tuner, we define hyperparameters we want to tune as well as the search space. Here, we chose to tune the the dimensionality of the user and movie embeddings, the number of dense layers in the deep portion of the model, the dropout rate, as well as the learning rate in the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hw0Q1oZ3fTbn"
      },
      "source": [
        "class EmbeddingLayer:\n",
        "    def __init__(self, n_items, n_factors):\n",
        "        self.n_items = n_items\n",
        "        self.n_factors = n_factors\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        x = Embedding(self.n_items, self.n_factors, embeddings_initializer='he_normal',\n",
        "                      embeddings_regularizer=l2(1e-6))(x)\n",
        "        x = Reshape((self.n_factors,))(x)\n",
        "        return x\n",
        "\n",
        "def WideDeepRecommender(hp):\n",
        "  \n",
        "    user = Input(shape=(1,))\n",
        "    u = EmbeddingLayer(n_users, hp.Int('n_factors_users', 10, 100, step=10, default=50))(user)\n",
        "    \n",
        "    movie = Input(shape=(1,))\n",
        "    m = EmbeddingLayer(n_movies, hp.Int('n_factors_movies', 10, 100, step=10, default=50))(movie)\n",
        "\n",
        "    genres = Input(shape=(20,))\n",
        "    \n",
        "    combinedembedding = Concatenate()([u, m])\n",
        "    x = Activation('relu')(combinedembedding)\n",
        "    \n",
        "    dropout_rate=hp.Float('dropout', 0, 0.90, step=0.1, default=0.5)\n",
        "    \n",
        "    for i in range(hp.Int('dense_blocks', 1, 3, default=2)):\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Dense(128/(2**i), kernel_initializer='he_normal', kernel_regularizer=l2(1e-6), activation='relu')(x)\n",
        "      x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    widedeep=Concatenate()([genres, x])\n",
        "    \n",
        "    x = Dense(1, kernel_initializer='he_normal')(widedeep)\n",
        "    x = Activation('sigmoid')(x)\n",
        "    x = Lambda(lambda x: x * (max_rating - min_rating) + min_rating)(x)\n",
        "    \n",
        "    model = Model(inputs=[user, movie, genres], outputs=x)\n",
        "    opt = Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
        "    \n",
        "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uod0tAO1c7Fk"
      },
      "source": [
        "We use Bayesian Optimization to search for the hyperparameters that minimize the validation RMSE. Only 5 trials of hyperparameter combinations are run due to GPU usage limit on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_gaLPliQkI5"
      },
      "source": [
        "tuner = kt.BayesianOptimization(WideDeepRecommender, kt.Objective(\"val_root_mean_squared_error\", direction=\"min\"), max_trials=5, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeXC2Gx7MYaA",
        "outputId": "29f59aec-46ca-41ac-898d-1de2e8408bd2"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 5\n",
            "n_factors_users (Int)\n",
            "{'default': 50, 'conditions': [], 'min_value': 10, 'max_value': 100, 'step': 10, 'sampling': None}\n",
            "n_factors_movies (Int)\n",
            "{'default': 50, 'conditions': [], 'min_value': 10, 'max_value': 100, 'step': 10, 'sampling': None}\n",
            "dropout (Float)\n",
            "{'default': 0.5, 'conditions': [], 'min_value': 0.0, 'max_value': 0.9, 'step': 0.1, 'sampling': None}\n",
            "dense_blocks (Int)\n",
            "{'default': 2, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
            "learning_rate (Float)\n",
            "{'default': 0.0001, 'conditions': [], 'min_value': 0.0001, 'max_value': 0.01, 'step': None, 'sampling': 'log'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw-_0-UkOdra",
        "outputId": "b4601f6e-34cb-4193-e3ee-10f71a929d7d"
      },
      "source": [
        "tuner.search(X_train_array, y_train, validation_split=0.2, epochs=100, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_root_mean_squared_error', patience=1)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 36m 46s]\n",
            "val_root_mean_squared_error: 1.0363560914993286\n",
            "\n",
            "Best val_root_mean_squared_error So Far: 0.8482723832130432\n",
            "Total elapsed time: 03h 15m 03s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXi74IQedLHh"
      },
      "source": [
        "Looking at the results below, it appears the best hyperparameter combination was n_factors_users: 100, n_factors_movies: 100, dropout: 0.0, dense_blocks: 1 ,learning_rate: 0.0001, which obtained a RMSE of 0.8483 on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA0bw8FVRs9i",
        "outputId": "4a213b35-d16a-4971-ba89-ccd81be143b1"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_root_mean_squared_error', direction='min')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_factors_users: 100\n",
            "n_factors_movies: 100\n",
            "dropout: 0.0\n",
            "dense_blocks: 1\n",
            "learning_rate: 0.0001\n",
            "Score: 0.8482723832130432\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_factors_users: 100\n",
            "n_factors_movies: 100\n",
            "dropout: 0.0\n",
            "dense_blocks: 3\n",
            "learning_rate: 0.0001\n",
            "Score: 0.8532862663269043\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_factors_users: 60\n",
            "n_factors_movies: 80\n",
            "dropout: 0.2\n",
            "dense_blocks: 2\n",
            "learning_rate: 0.0029594770617025457\n",
            "Score: 0.8740978837013245\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_factors_users: 30\n",
            "n_factors_movies: 20\n",
            "dropout: 0.7000000000000001\n",
            "dense_blocks: 2\n",
            "learning_rate: 0.0007719356053322489\n",
            "Score: 0.9145234227180481\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "n_factors_users: 100\n",
            "n_factors_movies: 100\n",
            "dropout: 0.9\n",
            "dense_blocks: 1\n",
            "learning_rate: 0.01\n",
            "Score: 1.0363560914993286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9GkgE0yy0_a"
      },
      "source": [
        "best_model = tuner.get_best_models(1)[0]\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIaUGR6rbe1B"
      },
      "source": [
        "best_model.save('best_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LseO_RrMeCkP"
      },
      "source": [
        "Now, let's see the performance of the best model on the entire trainset as well as the held out test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2474tducLbcH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31c590a-c822-40c7-905c-ead53c8d7915"
      },
      "source": [
        "uploadbestmodel=tf.keras.models.load_model('best_model.h5')\n",
        "testpredictions=uploadbestmodel.predict(X_test_array)\n",
        "trainpredictions=uploadbestmodel.predict(X_train_array)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fkVoTsTJsB3"
      },
      "source": [
        "testmetricmatrix = pd.DataFrame({'userId': X_test[:, 0], 'movieId': X_test[:, 1],'rating': y_test, 'prediction': testpredictions.flatten()}, columns=['userId', 'movieId', 'rating', 'prediction'])\n",
        "trainmetricmatrix = pd.DataFrame({'userId': X_train[:, 0], 'movieId': X_train[:, 1],'rating': y_train, 'prediction': trainpredictions.flatten()}, columns=['userId', 'movieId', 'rating', 'prediction'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPwC1Ez2LR-0"
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for index, row in predictions.iterrows(): \n",
        "        uid = row['userId']\n",
        "        user_est_true[uid].append((row['prediction'], row['rating']))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
        "\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    return precisions, recalls\n",
        "\n",
        "def get_top_recs(ratings, n):\n",
        "    #returns the top n recommendations \n",
        "    top_recs = {}\n",
        "    \n",
        "    for index, row in ratings.iterrows():\n",
        "        if row['userId'] not in top_recs.keys():\n",
        "            top_recs[row['userId']] = [(row['movieId'], row['prediction'], row['rating'])]\n",
        "        else:\n",
        "            top_recs[row['userId']].append((row['movieId'], row['prediction'], row['rating']))\n",
        "    \n",
        "    # sort the preds for each user and get the n highest ones.\n",
        "    for user, ratings in top_recs.items():\n",
        "        ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_recs[user] = ratings[:n]\n",
        "\n",
        "    return top_recs\n",
        "\n",
        "def user_coverage(top_recs, k):\n",
        "    user_cov = {}\n",
        "    tot = 0\n",
        "    for user in top_recs:\n",
        "        user_cov[user] = []\n",
        "        for ratings in top_recs[user]:\n",
        "            if ratings[2] >= 3.5:\n",
        "                user_cov[user].append(ratings[0])\n",
        "        if len(user_cov[user]) >= k:\n",
        "            tot += 1\n",
        "    return tot \n",
        "\n",
        "def item_coverage(top_recs, k):\n",
        "    item_cov = {}\n",
        "    tot = 0\n",
        "    for user in top_recs:\n",
        "        for ratings in top_recs[user]:\n",
        "            if ratings[0] not in item_cov.keys(): \n",
        "                item_cov[ratings[0]] = [] \n",
        "            if ratings[2] >= 3.5:\n",
        "                item_cov[ratings[0]].append(ratings[0])  \n",
        "                \n",
        "    for item in item_cov:\n",
        "        if len(item_cov[item]) >= k:\n",
        "            tot += 1        \n",
        "    return tot, item_cov\n",
        "\n",
        "def ndcg_at_k(predictions, k=10):\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    ndcgs = [] \n",
        "    for index, row in predictions.iterrows(): \n",
        "        uid = row['userId']\n",
        "        user_est_true[uid].append((row['prediction'], row['rating']))\n",
        "        \n",
        "    def dcg_at_k(user_ratings, user_preds, k=10):\n",
        "        # Sort user ratings by estimated value\n",
        "        user_preds = np.argsort(user_preds)[::-1]\n",
        "        user_ratings = np.take(user_ratings, user_preds[:k])\n",
        "        user_ratings = np.array(user_ratings)\n",
        "        num = 2 ** user_ratings - 1\n",
        "        den = np.log2(np.arange(2, num.size + 2))\n",
        "        dcg = np.sum(num/den)\n",
        "        return dcg\n",
        "        \n",
        "    for uid, scores in user_est_true.items():\n",
        "        user_preds = [x[0] for x in scores]\n",
        "        user_ratings = [x[1] for x in scores]\n",
        "        dcg = dcg_at_k(user_ratings, user_preds)\n",
        "        idcg = dcg_at_k(user_ratings, user_ratings)\n",
        "        ndcg = dcg/idcg\n",
        "        ndcgs.append(ndcg)\n",
        "        \n",
        "        \n",
        "    return ndcgs"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycurPsMKescT"
      },
      "source": [
        "## Train Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLBQyJSzevWC",
        "outputId": "5a535e15-6aa7-4fee-f0b3-ccefe493d86e"
      },
      "source": [
        "precisions_dl, recalls_dl = precision_recall_at_k(trainmetricmatrix, k=10, threshold=3.5)\n",
        "avg_dl_precision = sum(precisions_dl.values()) / len(precisions_dl)\n",
        "print(\"Average Precision @ 10:\", avg_dl_precision)\n",
        "avg_dl_recall = sum(recalls_dl.values()) / len(recalls_dl)\n",
        "print(\"Average Recall @ 10:\", avg_dl_recall)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision @ 10: 0.889979138321986\n",
            "Average Recall @ 10: 0.126984926967584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-m9Jn2OfQRe",
        "outputId": "a5a08ce4-95be-4246-83d6-c8420ce1a531"
      },
      "source": [
        "toprecs_dl_train = get_top_recs(trainmetricmatrix, 10)\n",
        "user_cov_train_dl = user_coverage(toprecs_dl_train, 1) / len(toprecs_dl_train)\n",
        "print (\"User Coverage of test set:\", user_cov_train_dl)\n",
        "item_cov_train_dl, item_dict_train_dl = item_coverage(toprecs_dl_train, 1)\n",
        "item_cov_train_dl = item_cov_train_dl / len(item_dict_train_dl)\n",
        "print (\"Item Coverage of test set:\", item_cov_train_dl)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User Coverage of test set: 0.9997142857142857\n",
            "Item Coverage of test set: 0.9522096608427544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BM4Qlb5fdaj",
        "outputId": "a459f6b5-1ba6-4892-8638-725f01638feb"
      },
      "source": [
        "ndcgs = ndcg_at_k(trainmetricmatrix)\n",
        "avg_ndcg = sum(ndcgs)/len(ndcgs)\n",
        "print(\"Average NDCG @ 10:\", avg_ndcg)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average NDCG @ 10: 0.7276305173808937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhmNkNGtfhVJ",
        "outputId": "aeb62503-cd29-4762-bafa-16743a63b309"
      },
      "source": [
        "print(\"Train RMSE:\",np.sqrt(mean_squared_error(y_train, trainpredictions.flatten())))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train RMSE: 0.80679214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmofOeJyewTv"
      },
      "source": [
        "## Test Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax2G42fRZM2g",
        "outputId": "8291cc59-e7ce-43e4-d1f2-179f9e8043fe"
      },
      "source": [
        "precisions_dl, recalls_dl = precision_recall_at_k(testmetricmatrix, k=10, threshold=3.5)\n",
        "avg_dl_precision = sum(precisions_dl.values()) / len(precisions_dl)\n",
        "print(\"Average Precision @ 10:\", avg_dl_precision)\n",
        "avg_dl_recall = sum(recalls_dl.values()) / len(recalls_dl)\n",
        "print(\"Average Recall @ 10:\", avg_dl_recall)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Precision @ 10: 0.8074136054421732\n",
            "Average Recall @ 10: 0.393811255416487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTvK1zyba8tm",
        "outputId": "57de33ae-6056-44d3-bc31-39ef2b211f35"
      },
      "source": [
        "toprecs_dl_test = get_top_recs(testmetricmatrix, 10)\n",
        "user_cov_test_dl = user_coverage(toprecs_dl_test, 1) / len(toprecs_dl_test)\n",
        "print (\"User Coverage of test set:\", user_cov_test_dl)\n",
        "item_cov_ts_dl, item_dict_ts_dl = item_coverage(toprecs_dl_test, 1)\n",
        "item_cov_test_dl = item_cov_ts_dl / len(item_dict_ts_dl)\n",
        "print (\"Item Coverage of test set:\", item_cov_test_dl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User Coverage of test set: 0.9981428571428571\n",
            "Item Coverage of test set: 0.8741970021413277\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO9b3eivh8dU",
        "outputId": "0eeee03f-1093-4804-c87c-2c8705f7c1ba"
      },
      "source": [
        "ndcgs = ndcg_at_k(testmetricmatrix)\n",
        "avg_ndcg = sum(ndcgs)/len(ndcgs)\n",
        "print(\"Average NDCG @ 10:\", avg_ndcg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average NDCG @ 10: 0.7441638893728827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_pU17tFuoF8",
        "outputId": "46af7cd9-1c5d-46ad-ef01-30921a88aed3"
      },
      "source": [
        "print(\"Test RMSE:\",np.sqrt(mean_squared_error(y_test, testpredictions.flatten())))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test RMSE: 0.84867543\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}